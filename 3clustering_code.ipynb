{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f51538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 11:25:40.002662: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-18 11:25:40.133710: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-18 11:25:40.746233: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib64:\n",
      "2024-02-18 11:25:40.746325: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib64:\n",
      "2024-02-18 11:25:40.746334: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method info in module logging:\n",
      "\n",
      "info(msg, *args, **kwargs) method of logging.Logger instance\n",
      "    Log 'msg % args' with severity 'INFO'.\n",
      "    \n",
      "    To pass exception information, use the keyword argument exc_info with\n",
      "    a true value, e.g.\n",
      "    \n",
      "    logger.info(\"Houston, we have a %s\", \"interesting problem\", exc_info=1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, ast\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import ast\n",
    "import pickle\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import logging\n",
    "from torch import Tensor, device\n",
    "from collections import deque\n",
    "from sklearn.cluster import KMeans\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "help(logger.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a95c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_load():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def data_processing(self):\n",
    "\n",
    "        df =pd.read_csv(INPUT_FILE)\n",
    "        df.drop_duplicates(INPUT_ID, inplace=True)\n",
    "        df = df.loc[df[INPUT_COLUMN_1].isnull()== False]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        try:\n",
    "            df[INPUT_COLUMN_1]=df[INPUT_COLUMN_1].apply(lambda x: ast.literal_eval(x))\n",
    "        except:\n",
    "            a=1\n",
    "\n",
    "        try:\n",
    "            df[INPUT_COLUMN_2]=df[INPUT_COLUMN_2].apply(lambda x: ast.literal_eval(x))\n",
    "        except:\n",
    "            a=1\n",
    "\n",
    "        try:\n",
    "            df[INPUT_COLUMN_3]=df[INPUT_COLUMN_3].apply(lambda x: ast.literal_eval(x))\n",
    "        except:\n",
    "            a=1\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def embeddings(self, final):\n",
    "\n",
    "        sentencemodel = SentenceTransformer(SENTENCE_MODEL_PATH)\n",
    "        col = list(range(EMBED_DIM)) + [INPUT_ID,'phrase_no']\n",
    "        X1 = pd.DataFrame(columns = col)\n",
    "\n",
    "        if final.shape[0] > 0:\n",
    "            for i in tqdm(range(final.shape[0])):\n",
    "                summary_sentences = final[INPUT_COLUMN_1].values[i]\n",
    "                if i==0:\n",
    "                    print('summary_sentences ', summary_sentences)\n",
    "                new_phrases = []\n",
    "                phrase_no = 0\n",
    "\n",
    "                if type(summary_sentences)==list:\n",
    "\n",
    "                    for c in range(len(summary_sentences)):\n",
    "                        t = summary_sentences[c].strip()\n",
    "                        phrase_embeddings = sentencemodel.encode(summary_sentences[c].strip(), device='cuda')\n",
    "                        phrase_embeddings = torch.tensor(phrase_embeddings)\n",
    "                        phrase_embeddings = phrase_embeddings.tolist()\n",
    "                        list_add = phrase_embeddings + [final.loc[i,INPUT_ID],phrase_no]\n",
    "                        phrase_no += 1\n",
    "                        X1.loc[len(X1)] = list_add\n",
    "\n",
    "        X1.to_pickle('./data_full_embeddings_smp.pickle')\n",
    "        return X1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c50e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ffdba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommunityDetection:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def cos_sim(self, a: Tensor, b: Tensor):\n",
    "\n",
    "        if not isinstance(a, torch.Tensor):\n",
    "            a = torch.tensor(a)\n",
    "\n",
    "        if not isinstance(b, torch.Tensor):\n",
    "            b = torch.tensor(b)\n",
    "\n",
    "        if len(a.shape) == 1:\n",
    "            a = a.unsqueeze(0)\n",
    "\n",
    "        if len(b.shape) == 1:\n",
    "            b = b.unsqueeze(0)\n",
    "\n",
    "        a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "        b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "        return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "    \n",
    "    def community_detection_modified_v2(self, embeddings, threshold, min_community_size, batch_size):\n",
    "        \"\"\"\n",
    "        Function for Fast Community Detection\n",
    "        Finds in the embeddings all communities, i.e. embeddings that are close (closer than threshold).\n",
    "        Returns only communities that are larger than min_community_size. The communities are returned\n",
    "        in decreasing order. The first element in each list is the central point in the community.\n",
    "        \"\"\"\n",
    "        mapping_row_index=dict(zip(range(len(embeddings)),list(embeddings.index)))\n",
    "\n",
    "        embeddings = torch.from_numpy(embeddings.to_numpy().astype('float64'))\n",
    "\n",
    "        if not isinstance(embeddings, torch.Tensor):\n",
    "            embeddings = torch.tensor(embeddings)\n",
    "\n",
    "        threshold = torch.tensor(threshold, device=embeddings.device)\n",
    "        extracted_communities = deque()\n",
    "\n",
    "        # Maximum size for community\n",
    "        min_community_size = min(min_community_size, len(embeddings))\n",
    "        sort_max_size = min(max(4 * min_community_size, 50), len(embeddings))\n",
    "\n",
    "        for start_idx in range(0, len(embeddings), batch_size):\n",
    "\n",
    "            # Compute cosine similarity scores\n",
    "            cos_scores = self.cos_sim(embeddings[start_idx:start_idx + batch_size], embeddings)\n",
    "\n",
    "            # Minimum size for a community\n",
    "            top_k_values, _ = cos_scores.topk(k=min_community_size, largest=True)\n",
    "\n",
    "            # Filter for rows >= min_threshold\n",
    "            for i in range(len(top_k_values)):\n",
    "\n",
    "                if top_k_values[i][-1] >= threshold:\n",
    "                    new_cluster = []\n",
    "\n",
    "                    # Only check top k most similar entries\n",
    "                    top_val_large, top_idx_large = cos_scores[i].topk(k=sort_max_size, largest=True)\n",
    "\n",
    "                    # Check if we need to increase sort_max_size\n",
    "                    while top_val_large[-1] > threshold and sort_max_size < len(embeddings):\n",
    "                        sort_max_size = min(2 * sort_max_size, len(embeddings))\n",
    "                        top_val_large, top_idx_large = cos_scores[i].topk(k=sort_max_size, largest=True)\n",
    "\n",
    "                    for idx, val in zip(top_idx_large.tolist(), top_val_large):\n",
    "                        if val < threshold:\n",
    "                            break\n",
    "\n",
    "                        new_cluster.append(idx)\n",
    "\n",
    "                    extracted_communities.append(new_cluster)\n",
    "\n",
    "            del cos_scores\n",
    "\n",
    "        extracted_communities=list(extracted_communities)\n",
    "        ## extracted_communities\n",
    "\n",
    "        # Largest cluster first\n",
    "        extracted_communities = sorted(extracted_communities, key=lambda x: len(x), reverse=True)\n",
    "\n",
    "        # Step 2) Remove overlapping communities\n",
    "        unique_communities = deque()\n",
    "        extracted_ids = set()\n",
    "        first_elem_list=deque()\n",
    "\n",
    "        for cluster_id, community in enumerate(extracted_communities):\n",
    "\n",
    "            non_overlapped_community = []      \n",
    "            first_elem=community[0]\n",
    "\n",
    "            if first_elem in extracted_ids:\n",
    "                continue\n",
    "            else:\n",
    "                for idx in community:\n",
    "                    if idx not in extracted_ids:\n",
    "                        non_overlapped_community.append(idx)\n",
    "\n",
    "            if len(non_overlapped_community) >= min_community_size:\n",
    "                first_elem_list.append([first_elem])\n",
    "                unique_communities.append(non_overlapped_community)\n",
    "                extracted_ids.update(non_overlapped_community)\n",
    "\n",
    "        unique_communities=list(unique_communities)\n",
    "        first_elem_list=list(first_elem_list)\n",
    "\n",
    "        unique_communities_upd=deque()\n",
    "        \n",
    "        for i, unique_comm in enumerate(unique_communities):\n",
    "\n",
    "            comm=[mapping_row_index[j] for j in unique_comm]\n",
    "            unique_communities_upd.append(comm)\n",
    "            \n",
    "\n",
    "\n",
    "        unique_communities=list(unique_communities_upd)\n",
    "        return unique_communities\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d60ec96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Clustering(CommunityDetection):\n",
    "    def __init__(self, df, X):\n",
    "        super().__init__()\n",
    "        \n",
    "        df=df.loc[df[INPUT_COLUMN_1].isnull()==False]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            df[INPUT_COLUMN_1]=df[INPUT_COLUMN_1].apply(lambda x: ast.literal_eval(x))\n",
    "        except:\n",
    "            a=1\n",
    "\n",
    "        try:\n",
    "            df[INPUT_COLUMN_2]=df[INPUT_COLUMN_2].apply(lambda x: ast.literal_eval(x))\n",
    "        except:\n",
    "            a=1\n",
    "\n",
    "        try:\n",
    "            df[INPUT_COLUMN_3]=df[INPUT_COLUMN_3].apply(lambda x: ast.literal_eval(x))\n",
    "        except:\n",
    "            a=1\n",
    "            \n",
    "        df['phrase_no']=df[INPUT_COLUMN_1].apply(lambda x: list(range(len(x))) )\n",
    "        \n",
    "        start=time.time()\n",
    "        df=df.explode([INPUT_COLUMN_1, INPUT_COLUMN_2, INPUT_COLUMN_3, 'phrase_no'])\n",
    "        end=time.time()\n",
    "        logger.info('Time taken in explode is %s %s ', (end-start), 'secs')\n",
    "\n",
    "        start=time.time()\n",
    "        df_info=df.loc[:, [INPUT_ID, INPUT_COLUMN_1, INPUT_COLUMN_2, INPUT_COLUMN_3,  'phrase_no', INPUT_TEXT]]\n",
    "        df_X=pd.merge(df_info, X, how ='inner', on=[INPUT_ID, 'phrase_no'])\n",
    "        df_X.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        long_description_list=df_X[INPUT_COLUMN_1].to_list()\n",
    "        verbose_label_list=df_X[INPUT_COLUMN_2].to_list()\n",
    "        succinct_label_list=df_X[INPUT_COLUMN_3].to_list()\n",
    "        \n",
    "        X=df_X.drop([INPUT_ID, INPUT_COLUMN_1, INPUT_COLUMN_2, INPUT_COLUMN_3,  'phrase_no', INPUT_TEXT], axis=1)\n",
    "        df_raw=df_X.loc[:, [INPUT_ID,INPUT_COLUMN_1, INPUT_COLUMN_2, INPUT_COLUMN_3, INPUT_TEXT]]\n",
    "    \n",
    "        end=time.time()\n",
    "        logger.info('Time taken in merge is %s %s', (end-start), 'secs')\n",
    "        \n",
    "        self.long_description_list=long_description_list\n",
    "        self.verbose_label_list=verbose_label_list\n",
    "        self.succinct_label_list=succinct_label_list\n",
    "        self.X=X\n",
    "        self.df_raw=df_raw\n",
    "        self.df_X=df_X\n",
    "        \n",
    "    def adding_clusters(self, indices_cluster, cluster_level, corresponding_level, df_raw_nc):\n",
    "        \n",
    "        long_description_list_nc=self.long_description_list.copy()\n",
    "        verbose_label_list_nc=self.verbose_label_list.copy()\n",
    "        succinct_label_list_nc=self.succinct_label_list.copy()\n",
    "        \n",
    "        X_nc=self.X.copy()\n",
    "        \n",
    "        if cluster_level=='0':\n",
    "            label=\"Cluster: \" + str(corresponding_level)\n",
    "        \n",
    "        elif cluster_level=='1':\n",
    "            index=indices_cluster[0]\n",
    "            label=succinct_label_list_nc[index]\n",
    "        \n",
    "        elif cluster_level=='2':\n",
    "            index=indices_cluster[0]\n",
    "            label=verbose_label_list_nc[index]\n",
    "        \n",
    "        else:\n",
    "            label=cluster_level\n",
    "        \n",
    "        #cluster_center_embeddings = X_nc.iloc[indices_cluster].mean().tolist()\n",
    "        cluster_long_description_list=[long_description_list_nc[ii] for ii in indices_cluster]\n",
    "        cluster_top_sentences=cluster_long_description_list[:min(3, len(cluster_long_description_list))]\n",
    "        \n",
    "        df_raw_nc.loc[indices_cluster, 'L'+cluster_level+'_cluster_no' ]=corresponding_level\n",
    "        df_raw_nc.loc[indices_cluster, 'L'+cluster_level+'_label' ]=label\n",
    "        \n",
    "        #df_raw_nc.loc[indices_cluster, 'L'+cluster_level+'_cluster_center_Embeddings' ]=str(cluster_center_embeddings)\n",
    "        df_raw_nc.loc[indices_cluster, 'L'+cluster_level+'_cluster_top_sentences' ]=str(cluster_top_sentences)\n",
    "        \n",
    "        return df_raw_nc\n",
    "\n",
    "        \n",
    "    def clustering(self,  L1_th, L2_th, min_L1, min_L2):\n",
    "        \n",
    "        X_f=self.X.copy()\n",
    "        df_raw_f=self.df_raw.copy()\n",
    "        \n",
    "        kmeans_clusters = max(int(X_f.shape[0]/5000)+1,2)\n",
    "    \n",
    "        logger.info('X %s', X_f.shape)\n",
    "\n",
    "        start_kmeans = time.time()\n",
    "        kmeans = KMeans(n_clusters=kmeans_clusters,random_state=0).fit(X_f)\n",
    "        end_kmeans = time.time()\n",
    "        logger.info(\"TIME TAKEN KMEANS: %s\",round(end_kmeans-start_kmeans,2))\n",
    "        target = kmeans.labels_\n",
    "\n",
    "\n",
    "        for k_searchval in range(kmeans_clusters):\n",
    "            start_b = time.time()\n",
    "            indices_L0 = [i for i in np.where(target == k_searchval)[0]]\n",
    "            logger.info('kmeans size %s',len(indices_L0) )\n",
    "\n",
    "            \n",
    "            cluster_level='0'\n",
    "            \n",
    "            corresponding_level=k_searchval\n",
    "            df_raw_f=self.adding_clusters(indices_L0, cluster_level, corresponding_level, df_raw_f )\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            X_L0=X_f.iloc[indices_L0]\n",
    "            clusters_L1=self.community_detection_modified_v2(X_L0, threshold=L1_th, min_community_size=min_L1, batch_size=X_f.shape[0] )\n",
    "\n",
    "            aa1 = time.time()\n",
    "            logger.info(\"At CD: %s\", str(round(aa1-start,2)) )\n",
    "\n",
    "            start_L1=time.time()            \n",
    "\n",
    "            for cluster_no_L1, c_L1 in enumerate(clusters_L1):\n",
    "                \n",
    "                cluster_level='1'\n",
    "                corresponding_level='L_'+str(k_searchval)+'_'+str(cluster_no_L1)\n",
    "                \n",
    "                df_raw_f=self.adding_clusters(c_L1, cluster_level, corresponding_level, df_raw_f)    \n",
    "                \n",
    "                X_L1 = X_f.iloc[c_L1]\n",
    "                clusters_L2 = self.community_detection_modified_v2(X_L1, threshold=L2_th, min_community_size=min_L2, batch_size=X_L1.shape[0] )\n",
    "\n",
    "                for cluster_no_L2, c_L2 in enumerate(clusters_L2):\n",
    "                    \n",
    "                    cluster_level='2'\n",
    "                    corresponding_level='L_'+str(k_searchval)+'_'+str(cluster_no_L1)+'_'+str(cluster_no_L2)\n",
    "                    df_raw_f=self.adding_clusters(c_L2, cluster_level, corresponding_level, df_raw_f)     \n",
    "\n",
    "            end_L1=time.time()\n",
    "\n",
    "            logger.info('Time taken in L1 is %s %s', (end_L1-start_L1), ' secs')\n",
    "        \n",
    "        df_raw_f['K_val']=df_raw_f['L0_cluster_no']\n",
    "         \n",
    "        return df_raw_f\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4c40246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering metric\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from torch import Tensor, device\n",
    "import time\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "class Clustering_metric():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        SENTENCE_MODEL_PATH = '/data2/bdlml/mkabra/pretrained_model/bge-large-en-v1.5' ## Path to Sentence Transformer Model\n",
    "        self.sentencemodel_bge = SentenceTransformer(SENTENCE_MODEL_PATH)\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def cos_sim(self, a: Tensor, b: Tensor):\n",
    "\n",
    "        if not isinstance(a, torch.Tensor):\n",
    "            a = torch.tensor(a)\n",
    "\n",
    "        if not isinstance(b, torch.Tensor):\n",
    "            b = torch.tensor(b)\n",
    "\n",
    "        if len(a.shape) == 1:\n",
    "            a = a.unsqueeze(0)\n",
    "\n",
    "        if len(b.shape) == 1:\n",
    "            b = b.unsqueeze(0)\n",
    "\n",
    "        a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "        b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "        return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "    \n",
    "    def cohesion_computation(self, cohesion_input ):\n",
    "        cohesion_calc=0\n",
    "        m=0\n",
    "        s=time.time()\n",
    "        for i in range(len(cohesion_input)):    \n",
    "            sent1_list=[cohesion_input[i]]\n",
    "            sent2_list=[]\n",
    "\n",
    "            for j in range(i+1, len(cohesion_input)):\n",
    "                sent2_list.append(cohesion_input[j])\n",
    "\n",
    "            if len(sent2_list)>=1:\n",
    "                sent1_embedding= self.sentencemodel_bge.encode(sent1_list)\n",
    "                sent2_embedding= self.sentencemodel_bge.encode(sent2_list)\n",
    "                sim=(self.cos_sim(sent1_embedding, sent2_embedding )).detach().numpy()[0]\n",
    "\n",
    "                #print(sim, sim.sum(), len(sim))\n",
    "                cohesion_calc+=sim.sum()\n",
    "                m+=len(sim)\n",
    "        e=time.time()\n",
    "        cohesion_calc=cohesion_calc/m\n",
    "        #print('cohesion_calc ', cohesion_calc)\n",
    "        #print('cohesion block ', (e-s), 'secs')\n",
    "        return cohesion_calc\n",
    "    \n",
    "    def topic_cohesion(self, df, pred_col, pred_col_label, input_var, k):\n",
    "        df_clustered=df.loc[:, [pred_col, pred_col_label, input_var]].groupby([pred_col, pred_col_label]).agg(tuple).applymap(list).reset_index()\n",
    "        pred_col_list=df_clustered[pred_col].to_list()\n",
    "        \n",
    "        all_clusters_cohesion=0\n",
    "        \n",
    "        \n",
    "        for n, col in enumerate(pred_col_list):\n",
    "            s=time.time()\n",
    "            \n",
    "                \n",
    "            label=df_clustered.loc[df_clustered[pred_col]==col][pred_col_label].to_numpy()\n",
    "            input_var_arr=df_clustered.loc[df_clustered[pred_col]==col][[input_var]].to_numpy()[0][0]\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                label_embeddings = self.sentencemodel_bge.encode(label)\n",
    "            except Exception as e:\n",
    "                print('Block label')\n",
    "                print(e)\n",
    "                print(n, col)\n",
    "                print(label)\n",
    "                print('\\n')\n",
    "            \n",
    "            try:\n",
    "                input_label_embeddings = self.sentencemodel_bge.encode(input_var_arr)\n",
    "            except Exception as e:\n",
    "                print('Block input_var_arr')\n",
    "                print(e)\n",
    "                print(n, col)\n",
    "                print(input_var_arr)\n",
    "                print('\\n')\n",
    "                \n",
    "            cos_scores=self.cos_sim(label_embeddings,input_label_embeddings )\n",
    "            \n",
    "           \n",
    "            top_k_values, indices = cos_scores.topk(k=min(k, len(input_label_embeddings)) , largest=True)\n",
    "           \n",
    "            cohesion_input=[]\n",
    "            for i in indices[0]:\n",
    "                cohesion_input.append(input_var_arr[i])\n",
    "\n",
    "            e=time.time()\n",
    "            \n",
    "            if len(cohesion_input)==1:\n",
    "                print('cohesion_input', cohesion_input, 'col ', col)\n",
    "            cluster_cohesion=self.cohesion_computation(cohesion_input)\n",
    "            \n",
    "            all_clusters_cohesion+=cluster_cohesion\n",
    "            \n",
    "            e2=time.time()\n",
    "            \n",
    "        n+=1\n",
    "        if n>0:\n",
    "            cohesion=all_clusters_cohesion/n\n",
    "        else:\n",
    "            cohesion='nis0'\n",
    "        \n",
    "        return cohesion\n",
    "   \n",
    "\n",
    "    def calculate_purity(self, true_col, pred_col, df): \n",
    "        '''\n",
    "        Calculate harmonic purity between two set of clusterings\n",
    "        df: a Pandas data frame containing two columns (true_col and pred_col)\n",
    "        true_col: column containing a ground-truth label for each document \n",
    "        pred_col: column containing a predicted label for each document\n",
    "        '''\n",
    "        contingency_matrix = metrics.cluster.contingency_matrix(df[true_col], df[pred_col]) \n",
    "        precision = contingency_matrix / contingency_matrix.sum(axis=0).reshape(1, -1)\n",
    "        recall = contingency_matrix / contingency_matrix.sum(axis=1).reshape(-1, 1)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        f1 = np.nan_to_num(f1)\n",
    "        purity = (np.amax(precision, axis=0) * contingency_matrix.sum(axis=0)).sum() / contingency_matrix.sum()\n",
    "        inverse_purity = (np.amax(recall, axis=1) * contingency_matrix.sum(axis=1)).sum() / contingency_matrix.sum()\n",
    "        harmonic_purity = (np.amax(f1, axis=1) * contingency_matrix.sum(axis=1)).sum() / contingency_matrix.sum()\n",
    "        return (purity, inverse_purity, harmonic_purity)\n",
    "    \n",
    "    def topic_diversion(self, df, pred_col, pred_col_label):\n",
    "        df_clustered_d=df.loc[:, [pred_col, pred_col_label]]\n",
    "        df_clustered_d.drop_duplicates(inplace=True)\n",
    "        all_labels=df_clustered_d[pred_col_label].to_numpy()\n",
    "        topic_diversion=self.cohesion_computation(all_labels)\n",
    "        return topic_diversion\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f7b4282",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE='./Parsed_CCP_Actions.csv'\n",
    "INPUT_COLUMN_1='ccp_long_description' \n",
    "INPUT_COLUMN_2='ccp_verbose_label'\n",
    "INPUT_COLUMN_3='ccp_succinct_label'\n",
    "INPUT_ID='case_no'\n",
    "INPUT_TEXT='trnscr_rdndnt_crosstalk_corr_rmv_tag'\n",
    "OUTPUT_FILE='./Clustered_file.csv'\n",
    "\n",
    "EMBED_DIM = 1024 \n",
    "SENTENCE_MODEL_PATH = './bge-large-en-v1.5' ## Path to Sentence Transformer Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83441179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2025 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary_sentences  [\"agent apologizes for the customer's frustrating experience and offers to help with the address.\", \"agent suggests writing a letter to the office of the president to address the customer's concerns.\", \"agent promises to look into the customer's concerns and provide a proper channel for their complaint.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2025/2025 [01:45<00:00, 19.24it/s]\n"
     ]
    }
   ],
   "source": [
    "data_load=Data_load()\n",
    "df=data_load.data_processing()\n",
    "X1=data_load.embeddings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18224ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.read_pickle('./data_full_embeddings_smp.pickle')\n",
    "master_clustering=Clustering(df, X)\n",
    "\n",
    "L1_th=0.7\n",
    "L2_th=0.8\n",
    "min_L1=5\n",
    "min_L2=2\n",
    "\n",
    "df_clustered=master_clustering.clustering(L1_th, L2_th, min_L1, min_L2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d15c6a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L0_label</th>\n",
       "      <th>L1_label</th>\n",
       "      <th>L2_label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Cluster: 0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">Credit Limit Reason</th>\n",
       "      <th>account review and confirmation</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>additional payment suggestion</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agent confirms if the cardmember is calling to increase or reinstate the limit.</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agent informs customer about the release program, a 12-month payment program that reduces the interest rate to 7.99% and allows the customer to make reduced monthly payments.</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apologize and acknowledge inconvenience</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Cluster: 1</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">Transfer</th>\n",
       "      <th>balance inquiry transfer</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone number collection</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supervisor connection</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transfer to billing dispute</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Verify Identity</th>\n",
       "      <th>verifying customer identity and card security code</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>237 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    0\n",
       "L0_label   L1_label            L2_label                                              \n",
       "Cluster: 0 Credit Limit Reason account review and confirmation                      8\n",
       "                               additional payment suggestion                       19\n",
       "                               agent confirms if the cardmember is calling to ...   4\n",
       "                               agent informs customer about the release progra...   2\n",
       "                               apologize and acknowledge inconvenience              2\n",
       "...                                                                                ..\n",
       "Cluster: 1 Transfer            balance inquiry transfer                            10\n",
       "                               phone number collection                              2\n",
       "                               supervisor connection                                4\n",
       "                               transfer to billing dispute                          2\n",
       "           Verify Identity     verifying customer identity and card security code   5\n",
       "\n",
       "[237 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clustered.groupby(['L0_label', 'L1_label', 'L2_label']).size().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05ba16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustered.to_csv(OUTPUT_FILE, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mk_ve_2",
   "language": "python",
   "name": "mk_ve_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
